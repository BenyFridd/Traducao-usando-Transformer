{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qP5J9u0km1x"
      },
      "source": [
        "Célula 1 – Instalação e Importação das Bibliotecas\n",
        "\n",
        "*   Item da lista\n",
        "*   Item da lista\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ySYzcPzpYnn"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow_datasets\n",
        "!pip install -U tensorflow-text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkZX85y78N8f"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "TIjABQakoO9e"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as tf_text\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAJ9-vPU8Phf"
      },
      "source": [
        "Carregamento do Dataset e Visualização de Exemplos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUGZWx9dou61",
        "outputId": "55ac7c8c-7971-4b97-beed-614d23c1d4a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Português: e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\n",
            "Inglês: and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
            "\n",
            "Português: mas e se estes fatores fossem ativos ?\n",
            "Inglês: but what if it were active ?\n",
            "\n",
            "Português: mas eles não tinham a curiosidade de me testar .\n",
            "Inglês: but they did n't test for curiosity .\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Carrega o dataset 'ted_hrlr_translate/pt_to_en' e separa em treino e validação\n",
        "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True, as_supervised=True)\n",
        "train_examples, val_examples = examples['train'], examples['validation']\n",
        "\n",
        "# Exibe 3 exemplos do dataset\n",
        "for pt, en in train_examples.take(3):\n",
        "    print(f'Português: {pt.numpy().decode(\"utf-8\")}')\n",
        "    print(f'Inglês: {en.numpy().decode(\"utf-8\")}\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gh3AFwX18RU2"
      },
      "source": [
        " Criação dos Tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "3pWvN4TmoxdP"
      },
      "outputs": [],
      "source": [
        "# Cria o tokenizer para o Português\n",
        "tokenizer_pt = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    (pt.numpy() for pt, _ in train_examples), target_vocab_size=2**13)\n",
        "\n",
        "# Cria o tokenizer para o Inglês\n",
        "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    (en.numpy() for _, en in train_examples), target_vocab_size=2**13)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLCNlZAX8Sej"
      },
      "source": [
        "Preparação dos Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "SMxkLPuEoyR7"
      },
      "outputs": [],
      "source": [
        "# Função para codificar as sentenças com tokens especiais de início e fim\n",
        "def encode(lang1, lang2):\n",
        "    lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(lang1.numpy()) + [tokenizer_pt.vocab_size+1]\n",
        "    lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(lang2.numpy()) + [tokenizer_en.vocab_size+1]\n",
        "    return lang1, lang2\n",
        "\n",
        "# Função que integra o tf.py_function para usar a função de encode\n",
        "def tf_encode(pt, en):\n",
        "    result_pt, result_en = tf.py_function(encode, [pt, en], [tf.int64, tf.int64])\n",
        "    result_pt.set_shape([None])\n",
        "    result_en.set_shape([None])\n",
        "    return result_pt, result_en\n",
        "\n",
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Prepara o dataset de treino\n",
        "train_dataset = train_examples.map(tf_encode)\n",
        "train_dataset = train_dataset.filter(lambda x, y: tf.logical_and(tf.size(x) <= 40, tf.size(y) <= 40))\n",
        "train_dataset = train_dataset.cache().shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Prepara o dataset de validação\n",
        "val_dataset = val_examples.map(tf_encode)\n",
        "val_dataset = val_dataset.filter(lambda x, y: tf.logical_and(tf.size(x) <= 40, tf.size(y) <= 40))\n",
        "val_dataset = val_dataset.padded_batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQCOjNos8UM7"
      },
      "source": [
        "Definição do Modelo Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "uarcrkABozVV"
      },
      "outputs": [],
      "source": [
        "# Função de codificação posicional\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = np.arange(position)[:, np.newaxis] / np.power(\n",
        "        10000, (2 * (np.arange(d_model)[np.newaxis, :] // 2)) / np.float32(d_model))\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "# Camada de atenção multi-cabeças\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        self.num_heads = num_heads\n",
        "        self.depth = d_model // num_heads\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        x = tf.reshape(x, (tf.shape(x)[0], -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        q = self.split_heads(self.wq(q))\n",
        "        k = self.split_heads(self.wk(k))\n",
        "        v = self.split_heads(self.wv(v))\n",
        "        scaled_attention, _ = scaled_dot_product_attention(q, k, v, mask)\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(scaled_attention, (tf.shape(scaled_attention)[0], -1, self.num_heads * self.depth))\n",
        "        return self.dense(concat_attention)\n",
        "\n",
        "# Função de atenção com produto escalar\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "    return output, attention_weights\n",
        "\n",
        "# Máscara de padding\n",
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "# Máscara para evitar ver tokens futuros\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask\n",
        "\n",
        "# Rede Feed Forward Pontual\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([tf.keras.layers.Dense(dff, activation='relu'),\n",
        "                                tf.keras.layers.Dense(d_model)])\n",
        "\n",
        "# Camada do Encoder\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, *, training=False, mask=None):\n",
        "        attn_output = self.mha(x, x, x, mask=mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "        return out2\n",
        "\n",
        "# Encoder completo\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
        "                 input_vocab_size, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(1000, d_model)\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
        "                           for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, *, training=False, mask=None):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        x = self.embedding(x)\n",
        "        x = tf.cast(x, tf.float32)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "        x = self.dropout(x, training=training)\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training=training, mask=mask)\n",
        "        return x\n",
        "\n",
        "# Camada do Decoder\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, *, training=False,\n",
        "             look_ahead_mask=None, padding_mask=None):\n",
        "        attn1 = self.mha1(x, x, x, mask=look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "        attn2 = self.mha2(enc_output, enc_output, out1, mask=padding_mask)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)\n",
        "        return out3\n",
        "\n",
        "# Decoder completo\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
        "                 target_vocab_size, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(1000, d_model)\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
        "                           for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, *, training=False,\n",
        "             look_ahead_mask=None, padding_mask=None):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        x = self.embedding(x)\n",
        "        x = tf.cast(x, tf.float32)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "        x = self.dropout(x, training=training)\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.dec_layers[i](x, enc_output, training=training,\n",
        "                                   look_ahead_mask=look_ahead_mask,\n",
        "                                   padding_mask=padding_mask)\n",
        "        return x\n",
        "\n",
        "# Modelo Transformer completo\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
        "                 input_vocab_size, target_vocab_size, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
        "                               input_vocab_size, rate)\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
        "                               target_vocab_size, rate)\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "    def call(self, inp, tar, *, training=False):\n",
        "        enc_padding_mask = create_padding_mask(inp)\n",
        "        look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "        dec_padding_mask = create_padding_mask(inp)\n",
        "        enc_output = self.encoder(inp, training=training, mask=enc_padding_mask)\n",
        "        dec_output = self.decoder(\n",
        "            tar, enc_output, training=training,\n",
        "            look_ahead_mask=look_ahead_mask, padding_mask=dec_padding_mask)\n",
        "        final_output = self.final_layer(dec_output)\n",
        "        return final_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adicionando a Métrica de Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Função para calcular a acurácia ignorando os tokens de padding (token 0)\n",
        "def compute_accuracy(tar_real, predictions):\n",
        "    predicted_ids = tf.argmax(predictions, axis=-1)\n",
        "    mask = tf.math.logical_not(tf.math.equal(tar_real, 0))\n",
        "    matches = tf.cast(tf.math.equal(predicted_ids, tar_real), tf.float32)\n",
        "    accuracy = tf.reduce_sum(matches * tf.cast(mask, tf.float32)) / tf.reduce_sum(tf.cast(mask, tf.float32))\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1StN7vdc8WWe"
      },
      "source": [
        "Configuração do Otimizador e Função de Perda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "G5hr_nQoo1Br"
      },
      "outputs": [],
      "source": [
        "learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=0.001, decay_steps=10000, decay_rate=0.96)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQj6QNQZ8Z03"
      },
      "source": [
        "Treinamento do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADRmEg4Ro100",
        "outputId": "78df3ac0-95c8-4dc1-befc-8dc0a758ffda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "Batch 0 Loss 9.0045\n",
            "Batch 50 Loss 6.2222\n",
            "Batch 100 Loss 5.7607\n",
            "Batch 150 Loss 5.3140\n",
            "Batch 200 Loss 5.0649\n",
            "Batch 250 Loss 4.9034\n",
            "Batch 300 Loss 4.6653\n",
            "Batch 350 Loss 4.8617\n",
            "Batch 400 Loss 4.4801\n",
            "Batch 450 Loss 4.4654\n",
            "Batch 500 Loss 4.2953\n",
            "Batch 550 Loss 4.3886\n",
            "Batch 600 Loss 4.2095\n",
            "Batch 650 Loss 4.1269\n",
            "Batch 700 Loss 4.0038\n",
            "Epoch 2/10\n",
            "Batch 0 Loss 4.0263\n",
            "Batch 50 Loss 3.6415\n",
            "Batch 100 Loss 3.8744\n",
            "Batch 150 Loss 3.8057\n",
            "Batch 200 Loss 3.7986\n",
            "Batch 250 Loss 3.4736\n",
            "Batch 300 Loss 3.5591\n",
            "Batch 350 Loss 3.7871\n",
            "Batch 400 Loss 3.3668\n",
            "Batch 450 Loss 3.5351\n",
            "Batch 500 Loss 3.3349\n",
            "Batch 550 Loss 3.4294\n",
            "Batch 600 Loss 3.6005\n",
            "Batch 650 Loss 3.4399\n",
            "Batch 700 Loss 3.3751\n",
            "Epoch 3/10\n",
            "Batch 0 Loss 3.1173\n",
            "Batch 50 Loss 3.1209\n",
            "Batch 100 Loss 2.8753\n",
            "Batch 150 Loss 2.9303\n",
            "Batch 200 Loss 3.1350\n",
            "Batch 250 Loss 2.9459\n",
            "Batch 300 Loss 2.8699\n",
            "Batch 350 Loss 3.3069\n",
            "Batch 400 Loss 2.8119\n",
            "Batch 450 Loss 2.7582\n",
            "Batch 500 Loss 2.8467\n",
            "Batch 550 Loss 2.8481\n",
            "Batch 600 Loss 2.8170\n",
            "Batch 650 Loss 2.8465\n",
            "Batch 700 Loss 2.8338\n",
            "Epoch 4/10\n",
            "Batch 0 Loss 2.6158\n",
            "Batch 50 Loss 2.4890\n",
            "Batch 100 Loss 2.2077\n",
            "Batch 150 Loss 2.4813\n",
            "Batch 200 Loss 2.3837\n",
            "Batch 250 Loss 2.1568\n",
            "Batch 300 Loss 2.4100\n",
            "Batch 350 Loss 2.3843\n",
            "Batch 400 Loss 2.5238\n",
            "Batch 450 Loss 2.3248\n",
            "Batch 500 Loss 2.5372\n",
            "Batch 550 Loss 2.3477\n",
            "Batch 600 Loss 2.5228\n",
            "Batch 650 Loss 2.4263\n",
            "Batch 700 Loss 2.2728\n",
            "Epoch 5/10\n",
            "Batch 0 Loss 2.1070\n",
            "Batch 50 Loss 2.3191\n",
            "Batch 100 Loss 2.1876\n",
            "Batch 150 Loss 2.0977\n",
            "Batch 200 Loss 2.0056\n",
            "Batch 250 Loss 2.1387\n",
            "Batch 300 Loss 1.9662\n",
            "Batch 350 Loss 2.0036\n",
            "Batch 400 Loss 1.9156\n",
            "Batch 450 Loss 2.2411\n",
            "Batch 500 Loss 1.8807\n",
            "Batch 550 Loss 2.0929\n",
            "Batch 600 Loss 1.9878\n",
            "Batch 650 Loss 2.1992\n",
            "Batch 700 Loss 2.1036\n",
            "Epoch 6/10\n",
            "Batch 0 Loss 1.6841\n",
            "Batch 50 Loss 2.0121\n",
            "Batch 100 Loss 1.9247\n",
            "Batch 150 Loss 1.8802\n",
            "Batch 200 Loss 2.0208\n",
            "Batch 250 Loss 1.9529\n",
            "Batch 300 Loss 1.8076\n",
            "Batch 350 Loss 1.9062\n",
            "Batch 400 Loss 1.8345\n",
            "Batch 450 Loss 1.8294\n",
            "Batch 500 Loss 1.7817\n",
            "Batch 550 Loss 1.9608\n",
            "Batch 600 Loss 2.0594\n",
            "Batch 650 Loss 1.9208\n",
            "Batch 700 Loss 2.0104\n",
            "Epoch 7/10\n",
            "Batch 0 Loss 1.5921\n",
            "Batch 50 Loss 1.7715\n",
            "Batch 100 Loss 1.5855\n",
            "Batch 150 Loss 1.7931\n",
            "Batch 200 Loss 1.8140\n",
            "Batch 250 Loss 1.8735\n",
            "Batch 300 Loss 1.8176\n",
            "Batch 350 Loss 1.6142\n",
            "Batch 400 Loss 1.6703\n",
            "Batch 450 Loss 1.7290\n",
            "Batch 500 Loss 1.8007\n",
            "Batch 550 Loss 1.8134\n",
            "Batch 600 Loss 1.9622\n",
            "Batch 650 Loss 1.7046\n",
            "Batch 700 Loss 1.9301\n",
            "Epoch 8/10\n",
            "Batch 0 Loss 1.5845\n",
            "Batch 50 Loss 1.7525\n",
            "Batch 100 Loss 1.6565\n",
            "Batch 150 Loss 1.6297\n",
            "Batch 200 Loss 1.4537\n",
            "Batch 250 Loss 1.5560\n",
            "Batch 300 Loss 1.6920\n",
            "Batch 350 Loss 1.7671\n",
            "Batch 400 Loss 1.5424\n",
            "Batch 450 Loss 1.5930\n",
            "Batch 500 Loss 1.7978\n",
            "Batch 550 Loss 1.6883\n",
            "Batch 600 Loss 1.6632\n",
            "Batch 650 Loss 1.6334\n",
            "Batch 700 Loss 1.6535\n",
            "Epoch 9/10\n",
            "Batch 0 Loss 1.4472\n",
            "Batch 50 Loss 1.3976\n",
            "Batch 100 Loss 1.4673\n",
            "Batch 150 Loss 1.3906\n",
            "Batch 200 Loss 1.4367\n",
            "Batch 250 Loss 1.4524\n",
            "Batch 300 Loss 1.6017\n",
            "Batch 350 Loss 1.3935\n",
            "Batch 400 Loss 1.5165\n",
            "Batch 450 Loss 1.5912\n",
            "Batch 500 Loss 1.5966\n",
            "Batch 550 Loss 1.4183\n",
            "Batch 600 Loss 1.5697\n",
            "Batch 650 Loss 1.5429\n",
            "Batch 700 Loss 1.5371\n",
            "Epoch 10/10\n",
            "Batch 0 Loss 1.4044\n",
            "Batch 50 Loss 1.4202\n",
            "Batch 100 Loss 1.4707\n",
            "Batch 150 Loss 1.2647\n",
            "Batch 200 Loss 1.3330\n",
            "Batch 250 Loss 1.4394\n",
            "Batch 300 Loss 1.3517\n",
            "Batch 350 Loss 1.5731\n",
            "Batch 400 Loss 1.2574\n",
            "Batch 450 Loss 1.4177\n",
            "Batch 500 Loss 1.5438\n",
            "Batch 550 Loss 1.4950\n",
            "Batch 600 Loss 1.5459\n",
            "Batch 650 Loss 1.4811\n",
            "Batch 700 Loss 1.5816\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Função que treina o modelo por uma época, calculando perda e acurácia\n",
        "def train_epoch(model, dataset, optimizer, loss_function):\n",
        "    \"\"\"\n",
        "    Treina o modelo por uma época inteira e calcula a perda e acurácia média.\n",
        "\n",
        "    Args:\n",
        "        model: O modelo Transformer.\n",
        "        dataset: Conjunto de dados de treinamento.\n",
        "        optimizer: Otimizador do TensorFlow.\n",
        "        loss_function: Função de perda utilizada.\n",
        "\n",
        "    Returns:\n",
        "        Tuple com a perda média e acurácia média da época.\n",
        "    \"\"\"\n",
        "    start = time.time()\n",
        "    total_loss = 0.0\n",
        "    total_accuracy = 0.0\n",
        "    batches = 0\n",
        "\n",
        "    for (batch, (inp, tar)) in enumerate(dataset):\n",
        "        tar_inp = tar[:, :-1]\n",
        "        tar_real = tar[:, 1:]\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(inp, tar_inp, training=True)\n",
        "            loss = loss_function(tar_real, predictions)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "        \n",
        "        # Calcula a acurácia para o batch atual\n",
        "        batch_accuracy = compute_accuracy(tar_real, predictions)\n",
        "        \n",
        "        total_loss += loss.numpy()\n",
        "        total_accuracy += batch_accuracy.numpy()\n",
        "        batches += 1\n",
        "        \n",
        "        if batch % 50 == 0:\n",
        "            print(f'Batch {batch}: Loss {loss.numpy():.4f}, Accuracy {batch_accuracy.numpy():.4f}')\n",
        "    \n",
        "    epoch_loss = total_loss / batches\n",
        "    epoch_accuracy = total_accuracy / batches\n",
        "    print(f'Epoch Loss: {epoch_loss:.4f}, Epoch Accuracy: {epoch_accuracy:.4f}, Time: {time.time() - start:.2f}s')\n",
        "    return epoch_loss, epoch_accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Função para Traduzir uma Frase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Função simples para traduzir uma frase usando o modelo treinado.\n",
        "def simple_translate(sentence):\n",
        "    \"\"\"\n",
        "    Traduz uma sentença de entrada utilizando o modelo Transformer treinado.\n",
        "    A implementação tokeniza a sentença de entrada e gera a tradução token a token.\n",
        "    \n",
        "    Args:\n",
        "        sentence: String com a sentença em Português.\n",
        "    \n",
        "    Returns:\n",
        "        String com a tradução prevista em Inglês.\n",
        "    \"\"\"\n",
        "    # Tokeniza a sentença e adiciona tokens de início e fim\n",
        "    tokenized_input = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(sentence) + [tokenizer_pt.vocab_size+1]\n",
        "    input_tensor = tf.expand_dims(tokenized_input, 0)\n",
        "    # Token inicial para a tradução em inglês\n",
        "    output = tf.expand_dims([tokenizer_en.vocab_size], 0)\n",
        "    \n",
        "    for i in range(40):\n",
        "        predictions = transformer(input_tensor, output, training=False)\n",
        "        predictions = predictions[:, -1, :]  # Pega o último token previsto\n",
        "        predicted_id = tf.argmax(predictions, axis=-1, output_type=tf.int32).numpy()[0]\n",
        "        if predicted_id == tokenizer_en.vocab_size+1:\n",
        "            break\n",
        "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
        "    \n",
        "    # Decodifica os tokens para formar a sentença traduzida\n",
        "    translated_sentence = tokenizer_en.decode([int(i) for i in output.numpy()[0] if i < tokenizer_en.vocab_size])\n",
        "    return translated_sentence\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
